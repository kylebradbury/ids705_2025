---
title: Schedule
subtitle: Spring 2025
# author:
#     - name: Kyle Bradbury
#       email: kyle.bradbury@duke.edu
#       affiliation: 
#       - name: Duke University
#         city: Durham
#         state: NC
#         url: www.duke.edu
# date: 'January 1, 2024'
toc: false
number-sections: false
---

The schedule below is a guide to what we will be covering throughout the semester and is subject to change to meet the learning goals of the class. Check this website regularly for the latest schedule and for course materials that will be posted here through links on the syllabus.

::: {.callout-tip}
## Key to books used below

- ISL = [**An Introduction to Statistical Learning with Python**](https://www.statlearning.com/), by James, Witten, Hastie, and Tibshirani
- UDL = [**Understanding Deep learning**](https://udlbook.github.io/udlbook/) by Simon Prince
- DM = Introduction to Data Mining, by Tan, Steinbach, Karpatne, and Kumar
- PRML = [**Pattern Recognition and Machine Learning**](https://www.microsoft.com/en-us/research/publication/pattern-recognition-machine-learning/), by Bishop
- DL = [**Deep Learning**](https://www.deeplearningbook.org/), by Goodfellow, Bengio, and Courville
- RL = [**Reinforcement Learning: An Introduction**](http://incompleteideas.net/book/the-book-2nd.html): An Introduction, by Sutton and Barto
:::

```{=html}
<table class="table table-hover">
  <thead class="thead-light">
    <tr>
      <th>Event Type</th>
      <th>Date</th>
      <th>Description</th>
      <th>Readings</th>
      <th style="min-width:120px">Course Materials</th>
    </tr>
  </thead>

  <tr>
    <td class="lecnum">Lecture 1</td>
    <td class="date">Thursday<br> Jan 9</td>
    <td>
      <b>What is machine learning?</b> <br>
      Course overview and an orientation to the major branches of machine learning: supervised, unsupervised, and reinforcement learning 
    </td>
    <td>None</td>
    <td>
      <!-- [slides] -->
      <a href="https://github.com/kylebradbury/ids705/raw/main/lectures/lecture01_what_is_machine_learning.pdf">[slides]</a><br>
    </td>
  </tr>

  <tr class="module">
    <td></td>
    <td></td>
    <td><b>Module 1: Supervised Learning</b></td>
    <td></td>
    <td></td>
  </tr>

  <tr class="table-primary">
    <td class="lecnum"></td>
    <td class="date">Tuesday<br> Jan 14</td>
    <td>
      <b>NO CLASS</b> <br>
      Make up on Friday 1/17
    </td>
    <td></td>
    <td></td>
  </tr>

  <tr>
    <td class="lecnum">Lecture 2</td>
    <td class="date">Thursday <br> Jan 16</td>
    <td>
      <b>An end-to-end machine learning example</b> <br>
      An introduction to formulating a supervised machine learning problem. Stating the problem, creating the model, evaluating performance, and operationalizing the solution. </td>
    <td>ISL Ch. 1 + 2.1<br><a href="https://warpwire.duke.edu/w/aWQHAA/">Watch this lecture</a> </td>
    <td>
      <!-- [slides] -->
      <a href="https://github.com/kylebradbury/ids705/raw/main/lectures/lecture02_end_to_end_machine_learning.pdf">[slides]</a><br>
      <a href="https://github.com/ageron/handson-ml/blob/master/02_end_to_end_machine_learning_project.ipynb">[sample code]</a>
    </td>
  </tr>

  <tr>
    <td class="lecnum">Lecture 3</td>
    <td class="date">Friday <br> Jan 17</td>
    <td>
      <b>How flexible should my algorithms be? The bias-variance tradeoff </b> <br>
      The bias-variance tradeoff explained using K-nearest neighbors classification
    </td>
    <td>ISL 2.2</td>
    <td>
      <!-- [slides] -->
      <a href="https://github.com/kylebradbury/ids705/raw/main/lectures/lecture03_bias_variance_tradeoff.pdf">[slides]</a>
    </td>
  </tr>

  <tr class="table-primary">
    <td><b></b></td>
    <td class="date">Monday <br> Jan 20</td>
    <td><b>Martin Luther King Jr. Day</b></td>
    <td></td>
    <td></td>
  </tr>

  <tr>
    <td class="lecnum">Lecture 4</td>
    <td class="date">Tuesday <br> Jan 21</td>
    <td>
      <b>Linear Models I</b> <br>
      Simple linear regression, multiple linear regression, measuring error, model fitting and least squares, comparing linear regression and classification
    </td>
    <td>ISL Intro of 3, 3.1, and 3.2</td>
    <td>
      <!-- [slides] -->
      <a href="https://github.com/kylebradbury/ids705/raw/main/lectures/lecture04_linear_models_1.pdf">[slides]</a>
    </td>
  </tr>

  <tr class="table-warning">va
    H:<td><b>Deliverable</b></td>
    <td class="date"> Wednesday <br> Jan 22</td>
    <td><b>Assignment #1 Due (at 9pm)</b><br> Probability, Linear Algebra, & Computational Programming</td>
    <td></td>
    <td>
      <a href="https://kylebradbury.github.io/ids705/notebooks/assignment1.html">[assignment]</a><br>
      <!-- [submit] -->
      <a href="https://www.gradescope.com/">[submit]</a>
    </td>
  </tr>

  <tr>
    <td class="lecnum">Lecture 5</td>
    <td class="date">Thursday <br> Jan 23</td>
    <td>
      <b>Linear Models II</b> <br>
      Nonlinear transformations of predictors; linear models for classification including the perceptron and logistic regression; cost/loss functions for classification (cross entropy loss); introduction to gradient descent.
    </td>
    <td>ISL 3.3 and 3.5</td>
    <td>
      <!-- [slides] -->
      <a href="https://github.com/kylebradbury/ids705/raw/main/lectures/lecture05_linear_models_2.pdf">[slides]</a>
    </td>
  </tr>
  
  <tr>
    <td class="lecnum">Lecture 6</td>
    <td class="date">Tuesday <br> Jan 28</td>
    <td>
      <b>Performance evaluation: metrics for regression/classification</b> <br>
      Choosing the right model: accuracy vs speed vs interpretability; metrics for supervised learning performance evaluation: types of errors, receiver operating characteristics curves, and confusion matrices
    </td>
    <td>ISL 4.1, 4.2, and 4.3</td>
    <td>
      <!-- [slides] -->
      <a href="https://github.com/kylebradbury/ids705/raw/main/lectures/lecture06_performance_evaluation_1.pdf">[slides]</a>
    </td>
  </tr>

  <tr>
    <td class="lecnum">Lecture 7</td>
    <td class="date">Thursday <br> January 30</td>
    <td>
      <b>Experimental designs for evaluating generalization performance and model comparison</b> <br>
      How to construct effective experimental designs for evaluating and comparing models; using model performance metrics to measure generalization performance; resampling techniques: training, testing, and validation datasets and cross validation; common pitfalls around biased sampling and data snooping/leakage
    </td>
    <td>ISL 5.1 and 5.2</td>
    <td>
      <!-- [slides] -->
      <a href="https://github.com/kylebradbury/ids705/raw/main/lectures/lecture07_performance_evaluation_2.pdf">[slides]</a>
    </td>
  </tr>

  <tr>
    <td class="lecnum">Lecture 8</td>
    <td class="date">Tuesday <br> Feb 4</td>
    <td>
      <b>Decision theory</b> <br>
      A risk-based framework for determining to operate supervised learning algorithms in practice; choosing ROC operating points through risk-minimization and how application-specific costs associated with different types of errors can be used to determine optimal operating points for classifiers
    </td>
    <td>
      <!-- Link to reading -->
      <a href="https://edstem.org/us/courses/69557/resources?download=50164">Link to reading</a>
    </td>
    <td>
      <!-- [slides] -->
      <a href="https://github.com/kylebradbury/ids705/raw/main/lectures/lecture08_decision_theory.pdf">[slides]</a>
    </td>
  </tr>

  <tr class="table-warning">
    <td><b>Deliverable</b></td>
    <td class="date"> Wednesday<br>Feb 5</td>
    <td><b>Assignment #2 Due (at 9pm)</b><br>Supervised Machine Learning Fundamentals</td>
    <td></td>
    <td>
      <!-- [assignment] <br> -->
      <a href="https://kylebradbury.github.io/ids705/notebooks/assignment2.html">[assignment]</a><br>
      <!-- [submit] -->
      <a href="https://www.gradescope.com/">[submit]</a>
    </td>
  </tr>

  <tr>  
    <td class="lecnum">Lecture 9</td>
    <td class="date">Thursday <br> Feb 6</td>
    <td>
      <b>Reducing overfit</b> <br>
      Feature selection; Occam’s razor; Subset selection; L1 (ridge), L2 (LASSO), and elastic net regularization; early stopping.
    </td>
    <td>ISL 6.1 and 6.2</td>
    <td>
      <!-- [slides] -->
      <a href="https://github.com/kylebradbury/ids705/raw/main/lectures/lecture09_regularization.pdf">[slides]</a>
    </td>
  </tr>

  <tr>
    <td class="lecnum">Lecture 10</td>
    <td class="date">Tuesday <br> Feb 11</td>
    <td>
      <b>Generative models for classification</b> <br>
      Generative vs discriminative models; naïve Bayes
    </td>
    <td>ISL 4.4 and 4.5</td>
    <td>
      <!-- [slides] -->
      <a href="https://github.com/kylebradbury/ids705/raw/main/lectures/lecture10_generative_classification.pdf">[slides]</a>
    </td>
  </tr>

  <tr>
    <td class="lecnum">Lecture 11</td>
    <td class="date">Thursday <br> Feb 13</td>
    <td>
      <b>Tree-based models and ensembles</b> <br>
      From decision trees to random forests: bagging, bootstrapping, and boosting
    </td>
    <td>ISL 8.1 and 8.2</td>
    <td>
      [slides]
      <!-- <a href="https://github.com/kylebradbury/ids705/raw/main/lectures/lecture11_trees_and_ensembles.pdf">[slides]</a> -->
    </td>
  </tr>

    <tr>
    <td class="lecnum">Lecture 12</td>
    <td class="date">Tuesday <br> Feb 18</td>
    <td>
      <b>Neural networks I</b> <br>
      Introduction to neural networks and representation learning; forward propagation, network architecture, and how to adapt to regression or classification problems
    </td>
    <td>UDL Ch 3: 3.1, 3.2; PRML Ch 5: 5.1 </td>
    <td>
      [slides]
      <!-- <a href="https://github.com/kylebradbury/ids705/raw/main/lectures/lecture13_neural_networks_1.pdf">[slides]</a> -->
    </td>
  </tr>

  <tr class="table-warning">
    <td><b>Deliverable</b></td>
    <td class="date"> Wednesday <br> Feb 19</td>
    <td><b>Assignment #3 Due (at 9pm)</b><br>Supervised learning model training and evaluation</td>
    <td></td>
    <td>
      <!-- [assignment] <br> -->
      <a href="https://kylebradbury.github.io/ids705/notebooks/assignment3.html">[assignment]</a><br>
      [submit]
      <!-- <a href="https://www.gradescope.com/">[submit]</a> -->
    </td>
  </tr>

  <tr>
    <td class="lecnum">Lecture 13</td>
    <td class="date">Thursday <br> Feb 20</td>
    <td>
      <b>Neural networks II</b> <br>
      Fitting a neural network to training data through gradient descent and backpropagation; how backpropagation is used to compute gradients in neural networks; hyperparameters and architecture choices in neural networks and practices for training neural networks warningfully
    </td>
    <td>UDL Ch 6: 6.1-6.2.2; PRML Ch 5: 5.3.1, and <a href="http://colah.github.io/posts/2015-08-Backprop/">Calculus on Computational Graphs</a></td>
    <td>
      [slides]
      <!-- <a href="https://github.com/kylebradbury/ids705/raw/main/lectures/lecture14_neural_networks_2.pdf">[slides]</a> -->
    </td>
  </tr>

  <tr>
    <td class="lecnum">Lecture 14</td>
    <td class="date">Tuesday <br> Feb 25</td>
    <td>
      <b>Introduction to Deep learning I</b> <br>
      Common architectures of deep learning models and the tools used to implement them
    </td>
    <td>UDL Ch 10: Convolutional Neural Networks</td>
    <td>
      [slides]
      <!-- <a href="https://github.com/kylebradbury/ids705/raw/main/lectures/lecture15_deep_learning.pdf">[slides]</a> -->
    </td>
  </tr>

  <tr>
    <td class="lecnum">Lecture 15</td>
    <td class="date">Thursday <br> Feb 27</td>
    <td>
      <b>Introduction to Deep learning II</b> <br>
      Common architectures of deep learning models and the tools used to implement them
    </td>
    <td>DL Ch 11: Practical Methodology<br>UDL Ch 12.1-12.5: Transformers</td>
    <td>
      [slides]
      <!-- <a href="https://github.com/kylebradbury/ids705/raw/main/lectures/lecture15_deep_learning.pdf">[slides]</a> -->
    </td>
  </tr>

  <tr class="module">
    <td></td>
    <td></td>
    <td><b>Module 2: Unsupervised Learning</b></td>
    <td></td>
    <td></td>
  </tr>

  <tr>
    <td class="lecnum">Lecture 16</td>
    <td class="date">Tuesday<br>Mar 4</td>
    <td>
      <b>Dimensionality reduction</b> <br>
      The Curse of Dimensionality and intro to principal components analysis (PCA)
    </td>
    <td>ISL 6.3, 6.4, 12.1, and 12.2 </td>
    <td>
      [slides]
      <!-- <a href="https://github.com/kylebradbury/ids705/raw/main/lectures/lecture16_dimensionality_reduction.pdf">[slides]</a> -->
    </td>
  </tr>

  <tr class="table-warning">
    <td><b>Deliverable</b></td>
    <td class="date">Wednesday <br> Mar 5</td>
    <td><b>Assignment #4 Due (at 9pm)</b><br>Neural Networks</td>
    <td></td>
    <td>
      [assignment] <br>
      [submit]
      <!-- <a href="https://github.com/kylebradbury/ids705/blob/main/assignments/Assignment_4.ipynb">[assignment]</a><br> -->
      <!-- <a href="https://www.gradescope.com/">[submit]</a><br> -->
      <!-- <a href="https://github.com/kylebradbury/neural-network-math/raw/master/neural_network_math.pdf">[NN Math Guide]</a> -->
    </td>
  </tr>

  <tr>
    <td class="lecnum">Lecture 17</td>
    <td class="date">Thursday<br>Mar 6</td>
    <td>
      <b>Principal components analysis (PCA)</b> <br>
      Explaining how PCA works and how we calculate the principal components.
    </td>
    <td>ISL 12.4</td>
    <td>
      [slides]
      <!-- <a href="https://github.com/kylebradbury/ids705/raw/main/lectures/lecture16_dimensionality_reduction.pdf">[slides]</a> -->
    </td>
  </tr>

  <tr class="table-warning">
    <td><b>Deliverable</b></td>
    <td class="date"> Friday <br> Mar 7 </td>
    <td><b>Project Proposal Due (at 9pm)</b></td>
    <td></td>
    <td>
      [project] <br>
      [submit]
      <!-- <a href="https://kylebradbury.github.io/ids705/project.html#proposal">[project]</a><br> -->
      <!-- <a href="https://www.gradescope.com/">[submit]</a> -->
    </td>
  </tr>

  <tr class="table-primary">
    <td></td>
    <td class="date">Mar 8-16</td>
    <td><b>Spring Break Week</b></td>
    <td></td>
    <td></td>
  </tr>

  <tr>
    <td class="lecnum">Lecture 18</td>
    <td class="date">Tuesday <br> Mar 18</td>
    <td>
      <b>Clustering I</b> <br>
      From K-means to Gaussian mixture model clustering and Expectation Maximization
    </td>
    <td>DM Ch 7 (<a href="https://www-users.cs.umn.edu/~kumar001/dmbook/ch7_clustering.pdf">link</a>): Intro, 7.1 and 7.2</td>
    <td>
      [slides]
      <!-- <a href="https://github.com/kylebradbury/ids705/raw/main/lectures/lecture18_density_estimation_and_clustering.pdf">[slides]</a> -->
    </td>
  </tr>

  <tr>
    <td class="lecnum">Lecture 19</td>
    <td class="date">Thursday <br> Mar 20</td>
    <td>
      <b>Clustering II</b> <br>
      Hierarchical clustering, DBSCAN, and spectral clustering
    </td>
   <td>DM Ch 7 (<a href="https://www-users.cs.umn.edu/~kumar001/dmbook/ch7_clustering.pdf">link</a>): 7.3 and 7.4</td>
   <td>
     [slides]
      <!-- <a href="https://github.com/kylebradbury/ids705/raw/main/lectures/lecture19_clustering_2.pdf">[slides]</a> -->
    </td>
  </tr>

  <tr class="module">
    <td></td>
    <td></td>
    <td><b>Module 3: Reinforcement Learning</b></td>
    <td></td>
    <td></td>
  </tr>

  <tr>
    <td class="lecnum">Lecture 20</td>
    <td class="date">Tuesday <br> Mar 25</td>
    <td>
      <b>Reinforcement Learning I</b> <br>
      Formulating the reinforcement learning problem
    </td>
    <td>RL Ch 1: 1.1-1.6; Ch 2: 2.1-2.5</td>
    <td>
      [slides]
      <!-- <a href="https://github.com/kylebradbury/ids705/raw/main/lectures/lecture20_reinforcement_learning_1.pdf">[slides]</a> -->
    </td>
  </tr>

  <tr class="table-warning">
    <td><b>Deliverable</b></td>
    <td class="date">Wednesday <br> Mar 26</td>
    <td><b>Assignment #5 Due (at 9pm)</b> <br> Kaggle Competition and Unsupervised Learning<br><b>Kaggle Competition Ends 9pm on Tuesday Mar 25</b></td>
    <td></td>
    <td>
      [assignment] <br>
      <!-- <a href="https://github.com/kylebradbury/ids705/blob/main/assignments/Assignment_5.ipynb">[assignment]</a><br> -->
      [submit]
      <!-- <a href="https://www.gradescope.com/">[submit]</a> -->
    </td>
  </tr>

  <tr>
    <td class="lecnum">Lecture 21</td>
    <td class="date">Thursday <br> Mar 27</td>
    <td>
      <b>Reinforcement Learning II</b> <br>
      Policy and value functions, rewards, and introduction to Markov processes 
    </td>
    <td>RL Ch 3</td>
    <td>
      [slides]
      <!-- <a href="https://github.com/kylebradbury/ids705/raw/main/lectures/lecture21_reinforcement_learning_2.pdf">[slides]</a> -->
    </td>
  </tr>

  <tr>
    <td class="lecnum">Lecture 22</td>
    <td class="date">Tuesday<br> Apr 1</td>
    <td>
      <b>Reinforcement Learning III</b> <br>
      From Markov Chains to Markov Decision Processes (MDPs)
    </td>
    <td>RL Ch 4</td>
    <td>
      [slides]
      <!-- <a href="https://github.com/kylebradbury/ids705/raw/main/lectures/lecture22_reinforcement_learning_3.pdf">[slides]</a> -->
    </td>
  </tr>

  <tr>
    <td class="lecnum">Lecture 23</td>
    <td class="date">Thursday <br> Apr 3</td>
    <td>
      <b>Reinforcement Learning IV</b> <br>
      Finding optimal policies through policy iteration, value iteration, and Monte Carlo methods
    </td>
    <td>RL Ch 5: 5.1-5.3</td>
    <td>
      [slides]
      <!-- <a href="https://github.com/kylebradbury/ids705/raw/main/lectures/lecture23_reinforcement_learning_4.pdf">[slides]</a> -->
    </td>
  </tr>

  <tr class="module">
    <td></td>
    <td></td>
    <td><b>Module 4: Practical Considerations and Advanced Topics</b></td>
    <td></td>
    <td></td>
  </tr>

  <tr>
    <td class="lecnum">Lecture 24</td>
    <td class="date">Tuesday <br> Apr 8</td>
    <td>
      <b>Practical Considerations and Advanced Topics I</b> <br>
      A survey of practical considerations and advanced topics
    </td>
    <td>None</td>
    <td>
      [slides]
      <!-- <a href="https://github.com/kylebradbury/ids705/raw/main/lectures/lecture24_special_topics.pdf">[slides]</a> -->
    </td>
  </tr>

  <tr>
    <td class="lecnum">Lecture 25</td>
    <td class="date">Thursday <br> Apr 10</td>
    <td>
      <b>Practical Considerations and Advanced Topics II</b> <br>
      A survey of practical considerations and advanced topics
    </td>
    <td>None</td>
    <td>
      [slides]
      <!-- <a href="https://github.com/kylebradbury/ids705/raw/main/lectures/lecture25_machine_learning_frontiers.pdf">[slides]</a> -->
    </td>
  </tr>

  <tr>
    <td class="lecnum">Lecture 26</td>
    <td class="date">Tuesday <br> Apr 15</td>
    <td>
      <b>Practical Considerations and Advanced Topics III</b> <br>
      A survey of practical considerations and advanced topics
    </td>
    <td>None</td>
    <td>
      [slides]
      <!-- <a href="https://github.com/kylebradbury/ids705/raw/main/lectures/lecture25_machine_learning_frontiers.pdf">[slides]</a> -->
    </td>
  </tr>

  <tr class="table-warning">
    <td><b>Deliverable</b></td>
    <td class="date">Wednesday <br> Apr 16 </td>
    <td><b>Draft Final Project Report Due (at 9pm)</b></td>
    <td></td>
    <td>
      [project] <br>
      [submit]
      <!-- <a href="https://kylebradbury.github.io/ids705/project.html#finalreport">[project]</a><br> -->
      <!-- <a href="https://www.gradescope.com/">[submit]</a> -->
    </td>
  </tr>

  <tr class="table-warning">
    <td><b>Deliverable</b></td>
    <td class="date">Wednesday <br> Apr 16</td>
    <td><b>(Optional) Assignment #6 Due (at 9pm)</b><br>Reinforcement learning</td>
    <td></td>
    <td>
      [assignment]
      <!-- <a href="https://github.com/kylebradbury/ids705/blob/main/assignments/Assignment_6.ipynb">[assignment]</a><br> -->
      <!-- <a href="https://www.gradescope.com/">[submit]</a> -->
    </td>
  </tr>

  <tr class="table-warning">
    <td><b>Deliverable</b></td>
    <td class="date">Wednesday <br> Apr 30 <br> 9am-noon</td>
    <td>
      <b>Final project showcase</b> <br>
      Meets during the final exam period<br><br> Due on this day:
      <ul>
        <li>Final Project Report (due by 9am on GradeScope)</li>
        <li>Final Project Presentation (due by 9am; submit link to presentation here)</li>
        <li>Project Github Repository (due by 9am on GradeScope)</li>
        <li>Final Project Peer Evaluation (due by 9pm; submit via TEAMMATES, see email)</li>
      </ul>
    </td>
    <td></td>
    <td>
      [project]
      <!-- <a href="https://kylebradbury.github.io/ids705/project.html">[project]</a> -->
    </td>
  </tr>

```